1 自动安装依赖  sudo apt-get install -f
2 查看存储空间  df -h
3 sudo apt install gdebi-core 
 4ubuntu 赋予文件权限
chmod 777 文件路径
5 修改文件权限
sudo chown -R zhang ./opt
6 sudo tar -zxvf 文件路径　-C　新文件路径　（-c：（create）建立打包文件）
7 linux运行jar包
java -jar XXX.jar
8 Linux命令后面加&
& 放在命令后面表示设置此进程为后台进程


export HBASE_HOME=/usr/local/hbase
export PATH=$PATH:$HBASE_HOME/bin
linux $PATH:的冒号是分隔符，类似windows的分号,$PATH引用PATH变量

scala
https://blog.csdn.net/zhousishuo/article/details/71598718
https://www.linuxidc.com/Linux/2016-12/137946.htm
maven
https://blog.csdn.net/qq_30581017/article/details/79217023
hadoop
https://blog.csdn.net/Ding_xiaofei/article/details/80376049


运行spark要hadoop jar包 要开启hadoop和spark

IDEA Scala 错误: 找不到或无法加载主类
https://blog.csdn.net/hongzhen91/article/details/81507140
https://blog.csdn.net/q2878948/article/details/85102502


错误:Exception in thread "main" java.lang.NoSuchMethodError: scala.Predef$.ArrowAssoc(Ljava/lang/Object;)
https://blog.csdn.net/u012102306/article/details/51103560

利用开发工具IntelliJ IDEA编写Spark应用程序
http://dblab.xmu.edu.cn/blog/1327/


spark maven打包java.lang.ClassNotFoundException
打包错误：打开jar看是否有编译好的.class
https://www.jianshu.com/p/b9c939bf2110


hive 元数据名字对应 和hive-site.xml


错误:Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot delete /user/zhang/output/ch07/randomsamplersort. Name node is in safe mode.

解决:hadoop fs -chmod -R 777 /


hdfs　查看二进制文件
hadoop dfs -text /user/zhang/outUserPartition2


错误：java.lang.ClassNotFoundException
一般是没有jar包


grep 匹配字符

可以在命令末尾加上
如　2>&1 | grep -E "Students|Row"
-E "A|B" 匹配多个字符
2>&1 是将标准出错重定向到标准输出,2是stderr 　1是stdout   只显示匹配到的那几行
2>&1>100 将内容写入到文件100中
 grep -E -o　"Students|Row"(多个参数可这样写)


改了　pom.xml  ｊar
导入jar包
加入两个　core.xml


提交jar包时的class 是object名称　　object a a改类名自动改

object名称改动　生成jar包时会自动覆盖掉
hbase 行键存在时，插入单元格时不会报错，直接在idea运行hbase插入语句时，可以插入成功，如果是int值，不会显示出来　　\x00\x00\x00



解决hbase 重启后数据丢失
权限
文件路径
要重启时要先停止hbase服务


启动hbase要先启动hadoop


DBeaver的配置


hive 的配置
要开启
netstat -anop|grep 10000
kill -9 7558
　hive --service hiveserver2

jar包在hive的jdbc文件中，要standlone的
要开启hadoop
hive的jdbc版本要对应

关闭hdfs安全模式
hdfs dfsadmin -safemode leave

https://blog.csdn.net/swing2008/article/details/53230145

hadoop 的core-site要改为　自己用户名

useSSL=false
https://blog.csdn.net/sunnyyoona/article/details/51648871


错误：java.net.ConnectException: Call From zhang-TravelMate-TX50-G2/127.0.1.1 to 0.0.0.0:8032 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused

将开启hadoop命令改为start-all.sh



hbase的数据是字节数组方式存储的



excel 用asci 码　　cvs用utf8
alt+enter 换行　alt+10
excel 文件被禁用掉，选项卡，加载项



少这样写　a =a['aa']　会出现keyword错误



hbase org.apache.hadoop.hbase.mapreduce.ImportTsv  -Dimporttsv.separator="," -Dimporttsv.columns=HBASE_ROW_KEY,info:company_financing_stage,info:company_industry,info:company_location,info:company_name,info:company_nature,info:company_overview,info:company_people,info:job_edu_require,info:job_exp_require,info:job_info,info:job_name,info:job_salary,info:job_tag,info:job_welfare jobs1 /part-m-00000



sudo mv apache-tomcat-8.5.9 /usr/tomcat/
sudo chmod 755 -R tomcat



kafka查看消费者
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic wordsendertest --from-beginning


ls -a查看所有文件(包括隐藏的）
图形界面中Ctrl-H 显示所有隐藏文件，再按一次取消

mysql 解决中文乱码
create database abcd DEFAULT CHARACTER SET utf8;



